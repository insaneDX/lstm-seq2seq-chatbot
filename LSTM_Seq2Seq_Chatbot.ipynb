{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0f8153c5",
      "metadata": {
        "id": "0f8153c5"
      },
      "source": [
        "Goal: Sequence-to-Sequence (Seq2Seq) chatbot using LSTM (Long Short-Term Memory) networks. The model learns to generate conversational responses by encoding input sentences and decoding them into output responses."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12470bfd",
      "metadata": {
        "id": "12470bfd"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "Step 0: Load necessary libraries and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3668436f",
      "metadata": {
        "id": "3668436f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "from collections import Counter\n",
        "import ast\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc51c9a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc51c9a7",
        "outputId": "1610917e-94ee-4835-d7d5-e8bc496d83da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/pandey881062/cornell-movie-dialogs-corpusdialog-datasets?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.58M/9.58M [00:01<00:00, 6.58MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/pandey881062/cornell-movie-dialogs-corpusdialog-datasets/versions/1\n",
            "Lines shape: (304713, 5)\n",
            "Conversations shape: (83097, 4)\n"
          ]
        }
      ],
      "source": [
        "# Download dataset from Kaggle\n",
        "path = kagglehub.dataset_download(\"pandey881062/cornell-movie-dialogs-corpusdialog-datasets\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# Load movie lines\n",
        "lines = pd.read_csv(\n",
        "    f\"{path}/movie_lines.txt\",\n",
        "    sep=r\" \\+\\+\\+\\$\\+\\+\\+ \",\n",
        "    engine=\"python\",\n",
        "    names=[\"lineID\", \"characterID\", \"movieID\", \"character_name\", \"text\"],\n",
        "    encoding=\"latin-1\"\n",
        ")\n",
        "\n",
        "# Load conversations\n",
        "conv = pd.read_csv(\n",
        "    f\"{path}/movie_conversations.txt\",\n",
        "    sep=r\" \\+\\+\\+\\$\\+\\+\\+ \",\n",
        "    engine=\"python\",\n",
        "    names=[\"char1ID\", \"char2ID\", \"movieID\", \"utterance_ids\"],\n",
        "    encoding=\"latin-1\"\n",
        ")\n",
        "\n",
        "print(\"Lines shape:\", lines.shape)\n",
        "print(\"Conversations shape:\", conv.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80b8154c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80b8154c",
        "outputId": "d72c2a5b-8dd2-40a2-b79e-18ca04ddfce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  lineID characterID movieID character_name          text\n",
            "0  L1045          u0      m0         BIANCA  They do not!\n",
            "1  L1044          u2      m0        CAMERON   They do to!\n",
            "2   L985          u0      m0         BIANCA    I hope so.\n",
            "3   L984          u2      m0        CAMERON     She okay?\n",
            "4   L925          u0      m0         BIANCA     Let's go.\n",
            "  char1ID char2ID movieID                     utterance_ids\n",
            "0      u0      u2      m0  ['L194', 'L195', 'L196', 'L197']\n",
            "1      u0      u2      m0                  ['L198', 'L199']\n",
            "2      u0      u2      m0  ['L200', 'L201', 'L202', 'L203']\n",
            "3      u0      u2      m0          ['L204', 'L205', 'L206']\n",
            "4      u0      u2      m0                  ['L207', 'L208']\n"
          ]
        }
      ],
      "source": [
        "# movie lines and movie conversations dataframes\n",
        "print(lines.head())\n",
        "print(conv.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "520142d6",
      "metadata": {
        "id": "520142d6"
      },
      "source": [
        "Step 1: Preprocessing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1dfe7c1",
      "metadata": {
        "id": "f1dfe7c1"
      },
      "outputs": [],
      "source": [
        "# Parse utterance IDs from string to list\n",
        "conv[\"utterance_ids\"] = conv[\"utterance_ids\"].apply(ast.literal_eval)\n",
        "\n",
        "# Create mapping from line ID to text\n",
        "id2line = dict(zip(lines[\"lineID\"], lines[\"text\"]))\n",
        "\n",
        "# Build dialogue arrays with safety check\n",
        "conv[\"dialogue\"] = conv[\"utterance_ids\"].apply(\n",
        "    lambda ids: [id2line[i] for i in ids if i in id2line]\n",
        ")\n",
        "\n",
        "# Text cleaning function\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z0-9?.!,¿']+\", \" \", text)\n",
        "    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d6fe586",
      "metadata": {
        "id": "8d6fe586"
      },
      "source": [
        "Build pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "335eebe7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "335eebe7",
        "outputId": "6ad06d6b-7e88-463a-c225-da8f38e59e6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total conversation pairs: 221282\n",
            "Train pairs: 40000\n",
            "Validation pairs: 5000\n",
            "Test pairs: 5000\n"
          ]
        }
      ],
      "source": [
        "# Build conversation pairs with safety check\n",
        "pairs = []\n",
        "for dialogue in conv[\"dialogue\"]:\n",
        "    if len(dialogue) >= 2:  # Safety check\n",
        "        for i in range(len(dialogue) - 1):\n",
        "            input_text = clean_text(dialogue[i])\n",
        "            target_text = clean_text(dialogue[i + 1])\n",
        "            # Only add if both are non-empty\n",
        "            if input_text and target_text:\n",
        "                pairs.append((input_text, target_text))\n",
        "\n",
        "print(f\"Total conversation pairs: {len(pairs)}\")\n",
        "\n",
        "# Shuffle and limit pairs\n",
        "random.shuffle(pairs)\n",
        "MAX_PAIRS = 50000\n",
        "pairs = pairs[:MAX_PAIRS]\n",
        "\n",
        "# Split into train/val/test\n",
        "train_ratio, val_ratio = 0.8, 0.1\n",
        "train_size = int(len(pairs) * train_ratio)\n",
        "val_size = int(len(pairs) * val_ratio)\n",
        "\n",
        "train_pairs = pairs[:train_size]\n",
        "val_pairs = pairs[train_size:train_size + val_size]\n",
        "test_pairs = pairs[train_size + val_size:]\n",
        "\n",
        "print(f\"Train pairs: {len(train_pairs)}\")\n",
        "print(f\"Validation pairs: {len(val_pairs)}\")\n",
        "print(f\"Test pairs: {len(test_pairs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de614c51",
      "metadata": {
        "id": "de614c51"
      },
      "source": [
        "Step 2: Vocabulary Building:\n",
        "\n",
        "We need to convert text into numerical tokens that the neural network can process. This involves building a vocabulary mapping between words and indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7446ae37",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7446ae37",
        "outputId": "588516e7-81c5-4824-8722-0314c1bb99e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 16147\n",
            "Sample tokens: ['<pad>', '<unk>', '<sos>', '<eos>', 'no', '!', 'now', 'be', 'logical', ',', 'bud', \"you're\", 'fuck', 'logic', 'yes']\n"
          ]
        }
      ],
      "source": [
        "# Build vocabulary from training data only\n",
        "def build_vocab(pairs, min_count=2):\n",
        "    word_counts = Counter()\n",
        "    for input_text, target_text in pairs:\n",
        "        word_counts.update(input_text.split())\n",
        "        word_counts.update(target_text.split())\n",
        "\n",
        "    # Special tokens\n",
        "    special_tokens = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
        "    vocab = special_tokens + [word for word, count in word_counts.items()\n",
        "                              if count >= min_count]\n",
        "\n",
        "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "\n",
        "    return word2idx, idx2word\n",
        "\n",
        "word2idx, idx2word = build_vocab(train_pairs, min_count=2)\n",
        "vocab_size = len(word2idx)\n",
        "\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Sample tokens: {list(word2idx.keys())[:15]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd8082a1",
      "metadata": {
        "id": "dd8082a1"
      },
      "source": [
        "Encoding Function :  converts text to numerical indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "784ed5c1",
      "metadata": {
        "id": "784ed5c1"
      },
      "outputs": [],
      "source": [
        "def encode_sentence(sentence, word2idx, max_len=25, add_sos=False, add_eos=False):\n",
        "    tokens = sentence.split()\n",
        "\n",
        "    if add_sos:\n",
        "        tokens = [\"<sos>\"] + tokens\n",
        "    if add_eos:\n",
        "        tokens = tokens + [\"<eos>\"]\n",
        "\n",
        "    ids = [word2idx.get(token, word2idx[\"<unk>\"]) for token in tokens]\n",
        "\n",
        "    # Padding or truncation\n",
        "    if len(ids) < max_len:\n",
        "        ids += [word2idx[\"<pad>\"]] * (max_len - len(ids))\n",
        "    else:\n",
        "        ids = ids[:max_len]\n",
        "        if add_eos:  # Ensure <eos> is at the end\n",
        "            ids[-1] = word2idx[\"<eos>\"]\n",
        "\n",
        "    return ids\n",
        "\n",
        "# Prepare sequences\n",
        "MAX_LEN = 25\n",
        "\n",
        "def prepare_data(pairs, word2idx, max_len):\n",
        "    input_seqs = []\n",
        "    target_seqs = []\n",
        "    for input_text, target_text in pairs:\n",
        "        input_seq = encode_sentence(input_text, word2idx, max_len)\n",
        "        target_seq = encode_sentence(target_text, word2idx, max_len,\n",
        "                                     add_sos=True, add_eos=True)\n",
        "        input_seqs.append(input_seq)\n",
        "        target_seqs.append(target_seq)\n",
        "    return input_seqs, target_seqs\n",
        "\n",
        "train_input_seqs, train_target_seqs = prepare_data(train_pairs, word2idx, MAX_LEN)\n",
        "val_input_seqs, val_target_seqs = prepare_data(val_pairs, word2idx, MAX_LEN)\n",
        "test_input_seqs, test_target_seqs = prepare_data(test_pairs, word2idx, MAX_LEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98ee0c99",
      "metadata": {
        "id": "98ee0c99"
      },
      "source": [
        "Dataset & DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0631a404",
      "metadata": {
        "id": "0631a404"
      },
      "outputs": [],
      "source": [
        "class ChatDataset(Dataset):\n",
        "    def __init__(self, input_seqs, target_seqs):\n",
        "        self.input_seqs = torch.tensor(input_seqs, dtype=torch.long)\n",
        "        self.target_seqs = torch.tensor(target_seqs, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_seqs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_seqs[idx], self.target_seqs[idx]\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "train_dataset = ChatDataset(train_input_seqs, train_target_seqs)\n",
        "val_dataset = ChatDataset(val_input_seqs, val_target_seqs)\n",
        "test_dataset = ChatDataset(test_input_seqs, test_target_seqs)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8505f2d7",
      "metadata": {
        "id": "8505f2d7"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba6baa31",
      "metadata": {
        "id": "ba6baa31"
      },
      "source": [
        "Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c008b8d0",
      "metadata": {
        "id": "c008b8d0"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0.5):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Embedding layer: converts token indices to dense vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # LSTM layer: processes sequence and produces hidden states\n",
        "        self.lstm = nn.LSTM(\n",
        "            embed_size,\n",
        "            hidden_size,\n",
        "            num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=False\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len)\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "        # embedded shape: (batch_size, seq_len, embed_size)\n",
        "\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        # hidden shape: (num_layers, batch_size, hidden_size)\n",
        "        # cell shape: (num_layers, batch_size, hidden_size)\n",
        "\n",
        "        return hidden, cell"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2a2ac41",
      "metadata": {
        "id": "b2a2ac41"
      },
      "source": [
        "Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa7394ca",
      "metadata": {
        "id": "aa7394ca"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0.5):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Same embedding structure as encoder\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # LSTM for generating output sequence\n",
        "        self.lstm = nn.LSTM(\n",
        "            embed_size,\n",
        "            hidden_size,\n",
        "            num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Linear layer to project to vocabulary size\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        # x shape: (batch_size,) - single token\n",
        "        x = x.unsqueeze(1)  # (batch_size, 1)\n",
        "\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "        # embedded shape: (batch_size, 1, embed_size)\n",
        "\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        # output shape: (batch_size, 1, hidden_size)\n",
        "\n",
        "        prediction = self.fc(output.squeeze(1))\n",
        "        # prediction shape: (batch_size, vocab_size)\n",
        "\n",
        "        return prediction, hidden, cell"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3272a15b",
      "metadata": {
        "id": "3272a15b"
      },
      "source": [
        "Seq2Seq Model : combines the Encoder and Decoder, managing the flow of information and implementing Teacher Forcing during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1926a45a",
      "metadata": {
        "id": "1926a45a"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.size(0)\n",
        "        trg_len = trg.size(1)\n",
        "        vocab_size = self.decoder.vocab_size\n",
        "\n",
        "        # Initialize outputs tensor\n",
        "        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(self.device)\n",
        "\n",
        "        # Encode the source sequence\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        # First decoder input is <sos> token\n",
        "        input_token = trg[:, 0]\n",
        "\n",
        "        # Generate output sequence one token at a time\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
        "            outputs[:, t] = output\n",
        "\n",
        "            # Teacher forcing: use actual target or predicted token\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input_token = trg[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c39b6cb8",
      "metadata": {
        "id": "c39b6cb8"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b610247",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b610247",
        "outputId": "4611a77c-3816-41fd-87d3-1f1fc489120e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Model has 23,907,091 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "# Device selection\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Hyperparameters\n",
        "EMBED_SIZE = 256      # Dimension of word embeddings\n",
        "HIDDEN_SIZE = 512     # LSTM hidden state size\n",
        "NUM_LAYERS = 2        # Number of LSTM layers\n",
        "DROPOUT = 0.5         # Dropout probability\n",
        "LEARNING_RATE = 0.001 # Adam learning rate\n",
        "NUM_EPOCHS = 50       # Maximum epochs\n",
        "CLIP_GRAD = 1.0       # Gradient clipping threshold\n",
        "\n",
        "# Initialize models\n",
        "encoder = Encoder(vocab_size, EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT).to(device)\n",
        "decoder = Decoder(vocab_size, EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT).to(device)\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "# Count parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Model has {count_parameters(model):,} trainable parameters\")\n",
        "\n",
        "# Optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                   factor=0.5, patience=3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=word2idx[\"<pad>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb1e4e68",
      "metadata": {
        "id": "cb1e4e68"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, optimizer, criterion, clip, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for src, trg in dataloader:\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg, teacher_forcing_ratio=0.5)\n",
        "\n",
        "        # Reshape for loss calculation\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:].reshape(-1, output_dim)  # Skip first timestep\n",
        "        trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping to prevent exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, trg in dataloader:\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "            output = model(src, trg, teacher_forcing_ratio=0)  # No teacher forcing\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[:, 1:].reshape(-1, output_dim)\n",
        "            trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb34f212",
      "metadata": {
        "id": "fb34f212"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFFLJDrcCy7x",
        "outputId": "508fe473-da4e-45cb-f197-e59e42052bbc"
      },
      "id": "lFFLJDrcCy7x",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec 10 11:33:59 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P0             32W /   70W |   14752MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cdfdf84",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cdfdf84",
        "outputId": "51735a0c-9780-4c93-ff3d-c554d3f8ab6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "============================================================\n",
            "Epoch 01/50 | Train Loss: 5.9879 | Val Loss: 5.5641 and took 94.0 seconds\n",
            "  ✓ Saved best model (val_loss: 5.5641)\n",
            "Epoch 02/50 | Train Loss: 5.5568 | Val Loss: 5.5396 and took 93.9 seconds\n",
            "  ✓ Saved best model (val_loss: 5.5396)\n",
            "Epoch 03/50 | Train Loss: 5.4808 | Val Loss: 5.5364 and took 94.0 seconds\n",
            "  ✓ Saved best model (val_loss: 5.5364)\n",
            "Epoch 04/50 | Train Loss: 5.4201 | Val Loss: 5.5836 and took 93.9 seconds\n",
            "Epoch 05/50 | Train Loss: 5.3653 | Val Loss: 5.5269 and took 93.8 seconds\n",
            "  ✓ Saved best model (val_loss: 5.5269)\n",
            "Epoch 06/50 | Train Loss: 5.3162 | Val Loss: 5.5304 and took 93.5 seconds\n",
            "Epoch 07/50 | Train Loss: 5.2845 | Val Loss: 5.5626 and took 93.8 seconds\n",
            "Epoch 08/50 | Train Loss: 5.2837 | Val Loss: 5.5251 and took 94.0 seconds\n",
            "  ✓ Saved best model (val_loss: 5.5251)\n",
            "Epoch 09/50 | Train Loss: 5.2434 | Val Loss: 5.5381 and took 93.9 seconds\n",
            "Epoch 10/50 | Train Loss: 5.2302 | Val Loss: 5.5167 and took 94.0 seconds\n",
            "  ✓ Saved best model (val_loss: 5.5167)\n",
            "Epoch 11/50 | Train Loss: 5.2264 | Val Loss: 5.5338 and took 93.8 seconds\n",
            "Epoch 12/50 | Train Loss: 5.2004 | Val Loss: 5.5433 and took 93.9 seconds\n",
            "Epoch 13/50 | Train Loss: 5.1912 | Val Loss: 5.5263 and took 94.0 seconds\n",
            "Epoch 14/50 | Train Loss: 5.2017 | Val Loss: 5.5257 and took 94.1 seconds\n",
            "Epoch 15/50 | Train Loss: 5.1655 | Val Loss: 5.5112 and took 94.2 seconds\n",
            "  ✓ Saved best model (val_loss: 5.5112)\n",
            "Epoch 16/50 | Train Loss: 5.1627 | Val Loss: 5.5123 and took 94.0 seconds\n",
            "Epoch 17/50 | Train Loss: 5.1293 | Val Loss: 5.5430 and took 93.9 seconds\n",
            "Epoch 18/50 | Train Loss: 5.1605 | Val Loss: 5.5091 and took 93.9 seconds\n",
            "  ✓ Saved best model (val_loss: 5.5091)\n",
            "Epoch 19/50 | Train Loss: 5.1196 | Val Loss: 5.5312 and took 94.0 seconds\n",
            "Epoch 20/50 | Train Loss: 5.1261 | Val Loss: 5.5162 and took 94.2 seconds\n",
            "Epoch 21/50 | Train Loss: 5.1188 | Val Loss: 5.5572 and took 94.0 seconds\n",
            "Epoch 22/50 | Train Loss: 5.1233 | Val Loss: 5.5120 and took 94.2 seconds\n",
            "Epoch 23/50 | Train Loss: 5.1134 | Val Loss: 5.5164 and took 93.9 seconds\n",
            "Epoch 24/50 | Train Loss: 5.1064 | Val Loss: 5.5278 and took 94.0 seconds\n",
            "Epoch 25/50 | Train Loss: 5.0879 | Val Loss: 5.5306 and took 94.2 seconds\n",
            "Early stopping at epoch 25\n",
            "============================================================\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "EARLY_STOP_PATIENCE = 7\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    start = time.time()\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, criterion, CLIP_GRAD, device)\n",
        "    val_loss = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    # Learning rate scheduling\n",
        "    scheduler.step(val_loss)\n",
        "    end = time.time()\n",
        "    print(f\"Epoch {epoch+1:02}/{NUM_EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} and took {end - start:.1f} seconds\")\n",
        "\n",
        "    # store losses\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # Save best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_loss': val_loss,\n",
        "        }, 'best_chatbot_model.pt')\n",
        "        print(f\"  ✓ Saved best model (val_loss: {val_loss:.4f})\")\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= EARLY_STOP_PATIENCE:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_losses, label=\"Train\")\n",
        "plt.plot(val_losses, label=\"Val\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "maYKspA3ziuW",
        "outputId": "4f322272-b5ae-4458-f300-17845271d02d"
      },
      "id": "maYKspA3ziuW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference & Chat"
      ],
      "metadata": {
        "id": "NEWMY2Ndw_wa"
      },
      "id": "NEWMY2Ndw_wa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e29a4e5",
      "metadata": {
        "id": "3e29a4e5"
      },
      "outputs": [],
      "source": [
        "def predict(model, sentence, word2idx, idx2word, max_len=25, device='cpu'):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Encode input sentence\n",
        "        cleaned = clean_text(sentence)\n",
        "        input_seq = encode_sentence(cleaned, word2idx, max_len)\n",
        "        input_tensor = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Get encoder outputs (context)\n",
        "        hidden, cell = model.encoder(input_tensor)\n",
        "\n",
        "        # Start with <sos> token\n",
        "        input_token = torch.tensor([word2idx[\"<sos>\"]], dtype=torch.long).to(device)\n",
        "\n",
        "        result = []\n",
        "        for _ in range(max_len):\n",
        "            output, hidden, cell = model.decoder(input_token, hidden, cell)\n",
        "            top1 = output.argmax(1)\n",
        "            word_idx = top1.item()\n",
        "            word = idx2word.get(word_idx, \"<unk>\")\n",
        "\n",
        "            if word == \"<eos>\":\n",
        "                break\n",
        "            if word not in [\"<pad>\", \"<sos>\", \"<unk>\"]:\n",
        "                result.append(word)\n",
        "\n",
        "            input_token = top1\n",
        "\n",
        "        return ' '.join(result) if result else \"I don't understand.\"\n",
        "\n",
        "\n",
        "def chat(model, word2idx, idx2word, device):\n",
        "    \"\"\"Interactive chat function\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Chatbot is ready! Type 'quit' to exit.\")\n",
        "    print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip()\n",
        "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        if not user_input:\n",
        "            continue\n",
        "\n",
        "        response = predict(model, user_input, word2idx, idx2word, MAX_LEN, device)\n",
        "        print(f\"Bot: {response}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Predictions"
      ],
      "metadata": {
        "id": "_80uckP9xF_L"
      },
      "id": "_80uckP9xF_L"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best model\n",
        "checkpoint = torch.load('best_chatbot_model.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
        "\n",
        "# Test sentences\n",
        "test_sentences = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"What is your name?\",\n",
        "    \"Do you love me?\",\n",
        "    \"Where are you from?\",\n",
        "    \"What do you think about life?\",\n",
        "    \"Tell me a joke\",\n",
        "    \"I am feeling sad today\",\n",
        "    \"What is the meaning of life?\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Sample Predictions:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    response = predict(model, sentence, word2idx, idx2word, MAX_LEN, device)\n",
        "    print(f\"Input: {sentence}\")\n",
        "    print(f\"Response: {response}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0Yr9Xucw5_m",
        "outputId": "bd5c6286-aa76-4098-fbc1-1f56eb2c100a"
      },
      "id": "_0Yr9Xucw5_m",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded best model from epoch 18\n",
            "\n",
            "============================================================\n",
            "Sample Predictions:\n",
            "============================================================\n",
            "Input: Hello, how are you?\n",
            "Response: i , . . . . . . . . . . . . . . . . . . . .\n",
            "\n",
            "Input: What is your name?\n",
            "Response: i , . . . . . . . . . . . . . . . . . . . .\n",
            "\n",
            "Input: Do you love me?\n",
            "Response: i , . . . . . . . . . . . . . . . . . . . .\n",
            "\n",
            "Input: Where are you from?\n",
            "Response: i , . . . . . . . . . . . . . . . . . . . .\n",
            "\n",
            "Input: What do you think about life?\n",
            "Response: i , . . . . . . . . . . . . . . . . . . . .\n",
            "\n",
            "Input: Tell me a joke\n",
            "Response: i , . . . . . . . . . . . . . . . . . . . .\n",
            "\n",
            "Input: I am feeling sad today\n",
            "Response: i , . . . . . . . . . . . . . . . . . . . .\n",
            "\n",
            "Input: What is the meaning of life?\n",
            "Response: i , . . . . . . . . . . . . . . . . . . . .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned = clean_text(\"hello\")\n",
        "input_seq = encode_sentence(cleaned, word2idx, MAX_LEN)\n",
        "input_tensor = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "hidden, cell = model.encoder(input_tensor)\n",
        "input_token = torch.tensor([word2idx[\"<sos>\"]], dtype=torch.long).to(device)\n",
        "\n",
        "output, hidden, cell = model.decoder(input_token, hidden, cell)\n",
        "probs = torch.softmax(output, dim=-1)\n",
        "topk = torch.topk(probs, 10)\n",
        "print(\"Top-10 token indices:\", topk.indices.tolist())\n",
        "print(\"Top-10 token probs:\", topk.values.tolist())\n",
        "print([idx2word[i] for i in topk.indices.tolist()[0]])"
      ],
      "metadata": {
        "id": "8264GwQJNfOp",
        "outputId": "89594d81-49c3-4c55-91e5-62862627d7be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "8264GwQJNfOp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-10 token indices: [[133, 122, 39, 4, 14, 653, 93, 153, 104, 376]]\n",
            "Top-10 token probs: [[0.09363508224487305, 0.044821154326200485, 0.03851831331849098, 0.035744525492191315, 0.023330410942435265, 0.023034755140542984, 0.022640379145741463, 0.02245296910405159, 0.0200455654412508, 0.019092876464128494]]\n",
            "['i', 'you', 'what', 'no', 'yes', 'oh', \"i'm\", \"it's\", 'yeah', 'well']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Greedy Prediction: Taking argmax and predicting best possible token with highest probability score."
      ],
      "metadata": {
        "id": "V1lWCJIoDFd8"
      },
      "id": "V1lWCJIoDFd8"
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, sentence, word2idx, idx2word, max_len=20, device='cpu'):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_seq = encode_sentence(clean_text(sentence), word2idx, max_len)\n",
        "        input_tensor = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device)\n",
        "        hidden, cell = model.encoder(input_tensor)\n",
        "        input_token = torch.tensor([word2idx.get('<sos>', 1)], dtype=torch.long).to(device)\n",
        "        result = []\n",
        "        for _ in range(max_len):\n",
        "            output, hidden, cell = model.decoder(input_token, hidden, cell)\n",
        "            top1 = output.argmax(1)\n",
        "            word = idx2word.get(top1.item(), '<unk>')\n",
        "            if word == '<eos>':\n",
        "                break\n",
        "            result.append(word)\n",
        "            input_token = top1\n",
        "        return ' '.join(result)"
      ],
      "metadata": {
        "id": "Lt1ob0WpOTYX"
      },
      "id": "Lt1ob0WpOTYX",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best model\n",
        "checkpoint = torch.load('best_chatbot_model.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
        "\n",
        "# Test sentences\n",
        "test_sentences = [\n",
        "    \"loves me or not, yes?\",\n",
        "    \"Hello, how are you?\",\n",
        "    \"What is your name?\",\n",
        "    \"Do you love me?\",\n",
        "    \"Where are you from?\",\n",
        "    \"What do you think about life?\",\n",
        "    \"Tell me a joke\",\n",
        "    \"I am feeling sad today\",\n",
        "    \"What is the meaning of life?\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Sample Predictions:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    response = predict(model, sentence, word2idx, idx2word, MAX_LEN, device)\n",
        "    print(f\"Input: {sentence}\")\n",
        "    print(f\"Response: {response}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjv-CDEDOWPC",
        "outputId": "4fe7f615-147e-4094-9be0-a0e50efddd75"
      },
      "id": "cjv-CDEDOWPC",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded best model from epoch 18\n",
            "\n",
            "============================================================\n",
            "Sample Predictions:\n",
            "============================================================\n",
            "Input: loves me or not, yes?\n",
            "Response: i , . . . . . . . . . . . . . . . . . . . .\n",
            "\n",
            "Input: Hello, how are you?\n",
            "Response: i , . . . . . . . . . . . . . . . . . . . .\n",
            "\n",
            "Input: What is your name?\n",
            "Response: i , . . . . . . . . . . . . . . . . . . . .\n",
            "\n",
            "Input: Do you love me?\n",
            "Response: i , . . . . . . . . . . . . . . . . . . . .\n",
            "\n",
            "Input: Where are you from?\n",
            "Response: i , . . . . . . . . . . . . . . . . . . . .\n",
            "\n",
            "Input: What do you think about life?\n",
            "Response: i , . . . . . . . . . . . . . . . . . . . .\n",
            "\n",
            "Input: Tell me a joke\n",
            "Response: i , . . . . . . . . . . . . . . . . . . . .\n",
            "\n",
            "Input: I am feeling sad today\n",
            "Response: i , . . . . . . . . . . . . . . . . . . . .\n",
            "\n",
            "Input: What is the meaning of life?\n",
            "Response: i , . . . . . . . . . . . . . . . . . . . .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using top K and top P sampling with temprature\n",
        "\n",
        "to compress and expand the probability distribution to select the next word and using banned token ids so EOS will not be predicted in start and having repetition panalty to encorouge model to generate new tokens."
      ],
      "metadata": {
        "id": "7J-cxyx6DVta"
      },
      "id": "7J-cxyx6DVta"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- Sampling utilities ---\n",
        "def top_k_top_p_filtering(logits, top_k=0, top_p=1.0, min_tokens_to_keep=1):\n",
        "    # logits: [vocab_size] (1D)\n",
        "    logits = logits.clone()\n",
        "\n",
        "    # Top-k\n",
        "    if top_k > 0:\n",
        "        top_k = min(top_k, logits.size(-1))\n",
        "        kth_vals, kth_idx = torch.topk(logits, top_k)\n",
        "        min_keep_val = kth_vals[-1]\n",
        "        logits[logits < min_keep_val] = -float(\"inf\")\n",
        "\n",
        "    # Nucleus (top-p)\n",
        "    if 0 < top_p < 1.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        probs = F.softmax(sorted_logits, dim=-1)\n",
        "        cumprobs = torch.cumsum(probs, dim=-1)\n",
        "\n",
        "        # Mask tokens where cumulative prob > top_p\n",
        "        mask = cumprobs > top_p\n",
        "        # Keep at least min_tokens_to_keep\n",
        "        if min_tokens_to_keep > 0:\n",
        "            mask[..., :min_tokens_to_keep] = False\n",
        "        sorted_logits[mask] = -float(\"inf\")\n",
        "\n",
        "        # Scatter back to original order\n",
        "        logits = torch.full_like(logits, -float(\"inf\"))\n",
        "        logits.scatter_(0, sorted_indices, sorted_logits)\n",
        "\n",
        "    return logits\n",
        "\n",
        "def sample_next_token(\n",
        "    logits,\n",
        "    temperature=0.8,\n",
        "    top_k=50,\n",
        "    top_p=0.9,\n",
        "    banned_token_ids=None,\n",
        "    repetition_penalty=1.1,\n",
        "    generated_ids=None,\n",
        "):\n",
        "    # logits: [vocab_size]\n",
        "    logits = logits.clone()\n",
        "\n",
        "    # Ban some tokens outright (e.g., <pad>, <sos>)\n",
        "    if banned_token_ids:\n",
        "        for tid in banned_token_ids:\n",
        "            if tid is not None and 0 <= tid < logits.numel():\n",
        "                logits[tid] = -float(\"inf\")\n",
        "\n",
        "    # Light repetition penalty for tokens already generated\n",
        "    if repetition_penalty and repetition_penalty > 1.0 and generated_ids:\n",
        "        seen = set(generated_ids)\n",
        "        for tid in seen:\n",
        "            if 0 <= tid < logits.numel():\n",
        "                logits[tid] = logits[tid] / repetition_penalty\n",
        "\n",
        "    # Temperature\n",
        "    temperature = max(1e-5, float(temperature))\n",
        "    logits = logits / temperature\n",
        "\n",
        "    # Filter by top-k / top-p\n",
        "    logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p, min_tokens_to_keep=1)\n",
        "\n",
        "    # Convert to probabilities\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "    # If all probs are zero/nan, fall back to argmax\n",
        "    if not torch.isfinite(probs).all() or probs.sum() <= 0:\n",
        "        return torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "    # Sample\n",
        "    next_token = torch.multinomial(probs, num_samples=1)  # shape [1]\n",
        "    return next_token"
      ],
      "metadata": {
        "id": "bzxXVURyRA7Q"
      },
      "id": "bzxXVURyRA7Q",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(\n",
        "    model,\n",
        "    sentence,\n",
        "    word2idx,\n",
        "    idx2word,\n",
        "    max_len=25,\n",
        "    device=None,\n",
        "    temperature=0.8,\n",
        "    top_k=50,\n",
        "    top_p=0.9,\n",
        "    min_len=3,\n",
        "    repetition_penalty=1.15,\n",
        "):\n",
        "    \"\"\"\n",
        "    Decodes with temperature + top-k/top-p sampling.\n",
        "    - Keeps <unk> (only drops <pad> and <sos> from the final string).\n",
        "    - Avoids early <eos> until min_len.\n",
        "    - Avoids repeating punctuation back-to-back.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Resolve device automatically if not provided\n",
        "    if device is None:\n",
        "        device = next(model.parameters()).device\n",
        "\n",
        "    # Special token ids\n",
        "    pad_id = word2idx.get(\"<pad>\", 0)\n",
        "    sos_id = word2idx.get(\"<sos>\")\n",
        "    eos_id = word2idx.get(\"<eos>\")\n",
        "    unk_id = word2idx.get(\"<unk>\")\n",
        "\n",
        "    # Punctuation ids (to avoid repeating the same punctuation)\n",
        "    punct_ids = [word2idx[t] for t in [\",\", \".\", \"!\", \"?\", \"¿\"] if t in word2idx]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Encode input\n",
        "        cleaned = clean_text(sentence)\n",
        "        input_seq = encode_sentence(cleaned, word2idx, max_len)\n",
        "        input_tensor = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Encode\n",
        "        hidden, cell = model.encoder(input_tensor)\n",
        "\n",
        "        # Start token\n",
        "        if sos_id is None:\n",
        "            # fallback if vocab didn’t include <sos>\n",
        "            sos_id = unk_id if unk_id is not None else 1\n",
        "        input_token = torch.tensor([sos_id], dtype=torch.long).to(device)\n",
        "\n",
        "        generated_ids = []\n",
        "        result_tokens = []\n",
        "\n",
        "        for t in range(max_len):\n",
        "            output, hidden, cell = model.decoder(input_token, hidden, cell)\n",
        "            # output: [batch=1, vocab_size]\n",
        "            logits = output.squeeze(0)  # [vocab_size]\n",
        "\n",
        "            # Ban base tokens each step\n",
        "            banned = {pad_id, sos_id}\n",
        "            # Avoid consecutive duplicate punctuation (e.g., \", , ,\")\n",
        "            if len(generated_ids) > 0 and generated_ids[-1] in punct_ids:\n",
        "                banned.add(generated_ids[-1])\n",
        "\n",
        "            # Avoid early EOS\n",
        "            if eos_id is not None and t < min_len:\n",
        "                banned.add(eos_id)\n",
        "\n",
        "            # Sample next token\n",
        "            next_token = sample_next_token(\n",
        "                logits,\n",
        "                temperature=temperature,\n",
        "                top_k=top_k,\n",
        "                top_p=top_p,\n",
        "                banned_token_ids=banned,\n",
        "                repetition_penalty=repetition_penalty,\n",
        "                generated_ids=generated_ids,\n",
        "            )\n",
        "            token_id = int(next_token.item())\n",
        "            generated_ids.append(token_id)\n",
        "\n",
        "            # Stop if EOS (after min_len)\n",
        "            if eos_id is not None and token_id == eos_id and t >= min_len:\n",
        "                break\n",
        "\n",
        "            # Convert to word; keep <unk> to avoid “disappearing” content\n",
        "            word = idx2word.get(token_id, \"<unk>\")\n",
        "            if word not in [\"<pad>\", \"<sos>\"]:\n",
        "                result_tokens.append(word)\n",
        "\n",
        "            input_token = next_token  # feed next\n",
        "\n",
        "        # Simple detokenization: remove space before punctuation\n",
        "        text = \" \".join(result_tokens)\n",
        "        text = (\n",
        "            text.replace(\" ,\", \",\")\n",
        "                .replace(\" .\", \".\")\n",
        "                .replace(\" !\", \"!\")\n",
        "                .replace(\" ?\", \"?\")\n",
        "                .replace(\" ’ \", \"’\")\n",
        "        )\n",
        "        return text if text.strip() else \"I don't understand.\""
      ],
      "metadata": {
        "id": "8yx0dVkeRJoz"
      },
      "id": "8yx0dVkeRJoz",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After loading checkpoint:\n",
        "checkpoint = torch.load('best_chatbot_model.pt', map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Try different temperatures / top-k / top-p if needed\n",
        "tests = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"What is your name?\",\n",
        "    \"Tell me a joke.\",\n",
        "    \"I am feeling sad today.\",\n",
        "    \"Where are you from?\",\n",
        "    \"What do you think about life?\",\n",
        "    \"loves me or not, yes?\",\n",
        "    \"how are you.\"\n",
        "]\n",
        "\n",
        "for s in tests:\n",
        "    print(\"You:\", s)\n",
        "    print(\"Bot:\", predict(model, s, word2idx, idx2word, MAX_LEN, device,\n",
        "                          temperature=0.8, top_k=50, top_p=0.9))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FewlhV2dRPeB",
        "outputId": "a120ca72-049d-4f24-febc-b3e2a4fe348b"
      },
      "id": "FewlhV2dRPeB",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: Hello, how are you?\n",
            "Bot: i?, i, you know. <unk> on your life.\n",
            "\n",
            "You: What is your name?\n",
            "Bot: you mean, i.\n",
            "\n",
            "You: Tell me a joke.\n",
            "Bot: no,.\n",
            "\n",
            "You: I am feeling sad today.\n",
            "Bot: . for your way?\n",
            "\n",
            "You: Where are you from?\n",
            "Bot: yes, then you, mr., i'm a here. it, but you're a that i need it <unk>!\n",
            "\n",
            "You: What do you think about life?\n",
            "Bot: you're a, you just go.\n",
            "\n",
            "You: loves me or not, yes?\n",
            "Bot: okay, but.?\n",
            "\n",
            "You: how are you.\n",
            "Bot: we can have you that, but i do. i would just be me to see my whole.\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}